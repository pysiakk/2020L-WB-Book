## Predicting code defects using interpretable static measures.

*Authors: Wojciech Bogucki, Tomasz Makowski, Dominik Rafacz (Warsaw University of Technology)*

### Abstract


### Introduction and Motivation

Since the very beginning of the computer revolution there have been attempts to increase efficiency in determining possible defects and failures in the code. An effective method to do so could bring many potential benefits by identifying such sites as early as at the code development stage and eliminating costly errors at the deployment stage. McCabe and Halstead proposed a set of measures that are based on static properties of the code (including basic values, e.g. number of lines of code or number of unique operators, as well as transformations of them, [-@mccabe76, -@halstead77]). In their hypotheses, they argue that these measures can significantly help to build models that predict the sensitive spots in program modules. However, it can be argued that the measures they propose are artificial, non-intuitive, and above all, not necessarily authoritative, not taking into account many aspects of the written code and program [@fenton97].

To support their hypotheses with, McCabe and Halstead collected information about the code used in NASA using scrapers and then used machine learning algorithms. In this article we use the above data sets to build a model that best predicts the vulnerability of the code to errors. We check whether static code measures (being transformations of basic predictors) significantly improve prediction results for the so-called white box models (e.g. trees and linear regression). Our goal is to build, using simple data transformations and easily explainable methods, a so-called white box machine learning model (e.g. tree or logistic regression) that will achieve results comparable to the black box model (such as neural networks or gradient boosting machines) used on data without advanced measures. We also want to compare the effectiveness of the measures proposed by McCabe and Halstead and compare them with the measures we have generated.

### Dataset

Our dataset comes from the original research of Halstead and McCabe. We obtained it by combining the sets from OpenML [@OpenML2013] and supplementing them with data from the PROMISE repository [@Sayyad-Shirabad+Menzies:2005].

It contains data collected from NASA systems written in *C* and *C++* languages. The data is in the form of a data frame containing more than 15000 records. Each record describes one "program module". -- with this generic term, the authors defined the simplest unit of functionality (in this case, these are functions). Each record is described with a set of predictors, which can be divided into several groups:

* Basic measures (such as number of lines of code, number of operands, etc.).
* McCabe's measures how complex the code is in terms of control flow and cross-references .
* Halstead's measures for general code readability.
* Target column (1 if module contains defects, 0 if not).
* Source column we added, specifying from which subsystem the module came (the original 5 datasets came from different systems).

In order to verify our hypotheses, we decided at the beginning to remove the Halstead's measures (which were transformations of the basic measures) from the collection to see if we are able to build an effective black box model without them. We also wanted to remove McCabe's measurements, but the basic measurements that he used to calculate his measurements are not preserved in the collection, so we decided to keep them.

There are not many records with openly missing data in the set (< 1%), however, the values of some columns raise doubts -- in the column containing information about the number of lines of code of a given module in many cases there is a value 0, which is not reliable. We consider 0 in this column to be missing data and remove records that have 0 here (almost 2000 records).

### Methodology

Our research consist of the following stages:

1. Data exploration.
2. Initial data preparation.
3. Building of black-box and white-box models and comparing them against the relevant measurements.
4. Repeating the cycle:
   
   (a) Improvement of white box models by modifying their parameters or data.  
   (b) Measuring the effectiveness of the models built.
   (c) Analysis of the resulting models.
   (d) Keeping or rejecting changes for further work.
   
5. Selection of the best white box model and final comparison with the black box model.

We use R programming language and popular machine learning project management packages - mlr and drake [@mlr, @drake]. 

#### Data exploration

At this stage, we are taking a closer look at what the data looks like and we are analyzing their distributions, gaps, correlations and simple relationships.

#### Initial data preparation

This stage consists mainly of merging the data sets, as mentioned earlier, and adding a source column (in fact, we added five indicator columns, which contain one-hot-encoded value, as models generally do not cope well with character columns). Since there were very few data gaps, we are imputing them with the median, because this method is effective and fast.

Imputation is necessary from the very beginning, as many models cannot cope with missing values. Since there were very few missing values, it does not affect significantly the result of those models that models that would still work. We are not carrying out further transformations at this stage because we do not want to disturb the results of the next stage.

#### Starting models

We're building models on this almost unaltered data. We are using one poorly interpretable model (black-box) -- random forest, specifically ranger package [@ranger], because it is fast and low-effort. Among well interpretable models (white-boxes) used in our work there are:

   * logistic regression,
   * decision tree,
   * k-nearest neighbors algorithm.
   
We train the models into data that we have divided into five folds with a similar distribution of the decision variable, on which we will perform cross-validation. Then we compare the results using commonly used measure -- AUC (Area Under Curve) [@auc], which not only assesses whether the observations are well classified, but also takes into account the likelihood of belonging to a class. We use AUC as the main comparative criterion of the models also in the further part of our work.

#### Improving white-boxes

This is a key part of our work. In the iterative cycle we use different methods to improve the quality of the white box models. After applying each of these methods, we check whether it has improved our score and possibly analyze the model, using statistical methods (residuals analysis) and explanatory machine learning (DALEX package [@DALEX]), to draw indications of what should be done next.

We are trying the following methods:

* **Tuning hyperparameters of models** -- Default hyperparameters for models are generally good, but in specific cases using specific hyperparameters may yield in better results, so we use model-based optimization for tuning those parameters [@mlrmbo].
* **Logarithmic and exponential transformations of individual variables** -- So that linear relationships can be better captured and to reduce the influence of outliers, we transform variables using exponential and polynomial functions.
* **Discretization of continuous features** -- Some variables do not have a linear effect on the response variable, even if they are transformed by simple functions like exponential function, sometimes there are clear thresholds -- so we can replace the variable with indexes of individual segments. The SAFE algorithm helps with this [@gosiewska2019safe].
* **Generating new columns as functions of other columns** -- There may be interactions between variables that cannot be captured by linear models. In order to take them into account, we generate new columns, applying to the rest of them various transformations -- we take their inverses, products, quotients, elevations to power, and so on. As a result of these operations, a lot of new predictors are created, which we later evaluate. We also analyze their interpretability, i.e. to what extent they are translatable into an intuitive understanding of such a measure. At this point we also consider Halstead and McCabe's measurements. 
* **Oversampling and undersampling** -- On the basis of the data set, we generate more observations from the minority class using the SMOTE algorithm [@smote] and remove some of the observations from the majority class so that the model more emphasizes the differences in characteristics of individual classes.
* **Reducing outliers** -- For each variable a two-value vector that indicates the thresholds for which the values are considered as outliers is generated. Then all outliers are changed to the nearest value of obtained earlier vector.


Our goal is to beat black-box model. In our case we chose random forest model from package ranger. As a white-box model we used logistic regression. Results were tested on dataset with different transformations.
```{r echo=FALSE, results='asis'}
library(knitr)
kable(data.frame(red=c('no','yes','yes','no'),disc=c('no','no','no','yes'),log=c('no','no','yes','no'),ranger=c(0.7907, 0.7897,0.7782, 0.7593),logreg=c(0.7354,0.7431,0.7435,0.7305)),col.names=c("Outliers reduction","Discretization","Logarithm","Ranger","Logistic regression"), caption="White-box and black-box comparison")

```

#### Selecting the best model

At the end of the process, we select the model that has the highest AUC score for crossvlidation on our dataset.

### Results

### Summary and conclusions